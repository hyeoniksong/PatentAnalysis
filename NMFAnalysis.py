import pandas as pdfrom sklearn.decomposition import NMFfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction import text from nltk import word_tokenize, pos_tagfrom pywsd.utils import lemmatize_sentenceimport reimport stringimport matplotlib.pyplot as plt# Uncomment these if running for the first time#import nltk#nltk.download('averaged_perceptron_tagger')#nltk.download('punkt')#nltk.download('wordnet')# Plot topic results def plot_top_words(model, feature_names, n_top_words, title, ):    fig, axes = plt.subplots(1, 5, figsize=(30, 3), sharex=True)    axes = axes.flatten()    for topic_idx, topic in enumerate(model.components_):        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]        top_features = [feature_names[i] for i in top_features_ind]        weights = topic[top_features_ind]        ax = axes[topic_idx]        ax.barh(top_features, weights, height=0.7)        ax.set_title(f"Topic {topic_idx +1}", fontdict={"fontsize": 20})        ax.invert_yaxis()        ax.tick_params(axis="both", which="major", labelsize=15)        for i in "top right left".split():            ax.spines[i].set_visible(False)        fig.suptitle(title, fontsize=25, y=1.15)    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.50, hspace=0.3)    plt.show()#Choose your inputs here: tech = 'drone'minDate = 2020  maxDate = minDate + 1n_top_words = 10#Import document datadocuments = pd.read_csv("patentInfo_%s.csv" %(tech))# Clean Up Indexdocuments = documents.reset_index()# Select only President's Names and their Speechesdocuments = documents[['PatentNum', 'Date', 'Text']]# Set Index to President's Namesdocuments = documents.set_index('PatentNum')# Choose documents of a time windowdocuments = documents.loc[(documents['Date']>=minDate) & (documents['Date']<=maxDate)]documents = documents.drop(columns='Date')def clean_text_round1(text):    '''Make text lowercase, remove text in square brackets,     remove punctuation, remove read errors,    and remove words containing numbers.'''    text = text.lower()    text = re.sub('\[.*?\]', ' ', text)    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)    text = re.sub('\w*\d\w*', ' ', text)    text = re.sub('�','†' ' ', text)    text = re.sub(r'[^\w]', ' ', text)        return textround1 = lambda x: clean_text_round1(x)# Clean Speech Textdocuments["Text"] = documents["Text"].apply(round1)def clean_text_round2(text):    '''Lemmatize text '''    lemmatized = lemmatize_sentence(text)    return ' '.join(lemmatized)round2 = lambda x: clean_text_round2(x)documents["Text"] = documents["Text"].apply(round2)def pos_filter(text):    '''Filter by POS selected '''    '''Avaialble POS: NN - Noun, VB - Verb, JJ - Adjective, RB - Adverb'''    is_pos = lambda pos: pos[:2] == 'NN'     tokenized = word_tokenize(text)    all_pos = [word for (word, pos) in pos_tag(tokenized) if is_pos(pos)]         #return string of joined list of POS filtered text    return ' '.join(all_pos)# Create dataframe of only POS from speechesdocuments_pos = pd.DataFrame(documents.Text.apply(pos_filter))#additional_stopwords = ['nut', 'shell','nutcracker']#additional_stopwords = ['drone','uav','unmanned','aerial','vehicle']#additional_stopwords = ['apparatus','invention','device','say','mean','mechanism']additional_stopwords = ''stop_words_noun_agg = text.ENGLISH_STOP_WORDS.union(additional_stopwords)vect = TfidfVectorizer(stop_words=stop_words_noun_agg, min_df=2)# Fit and transform # Use 'documents' if not filtering by POS. Use, 'documents_pos' if filtering by POSX = vect.fit_transform(documents.Text)# Create an NMF instance: model# n_components is the number of topcismodel = NMF(n_components=5, random_state=5) # Fit the model to TF-IDFmodel.fit(X) # Transform the TF-IDF: nmf_features (this is document by topic matrix)nmf_features = model.transform(X)# Create a DataFrame: components_df (this is topic by word matrix)components_df = pd.DataFrame(model.components_, columns=vect.get_feature_names())result = pd.DataFrame()for topic in range(components_df.shape[0]):    tmp = components_df.iloc[topic]    print(f'For topic {topic+1} the words with the highest value are:')    print(tmp.nlargest(10))    print('\n')        topTen = tmp.nlargest(10)    topTen = topTen.reset_index(0)    result = pd.concat([result,topTen], axis=1)    result = result.rename(columns={'index':'Words'})    tfidf_feature_names = vect.get_feature_names_out()plot_top_words(model, tfidf_feature_names, n_top_words, "%s: %d - %d" %(tech,minDate,maxDate))check = pd.DataFrame(nmf_features).idxmax(axis = 1)